# 基于kubeadm 1.20.7部署kubernetes高可用集群

**一、环境描述** 

**1、节点说明** 

```
192.168.140.13	k8s-master01.linux.com
192.168.140.14	k8s-master02.linux.com
192.168.140.15	k8s-master03.linux.com
192.168.140.16	k8s-node01.linux.com
192.168.140.18	k8s-node02.linux.com
192.168.140.100	master VIP
```

**2、软件版本**

```
系统: CentOS 7.6
kubeadm版本: 1.20.7
docker版本: 19.03
Pod网段: 172.168.0.0/16
Server网段: 10.96.0.0/16
```

**二、基础环境配置**

**1、所有节点配置免密SSH**

**2、关闭所有节点SELinux、防火墙、时间同步**

```
[root@k8s-master01 ~]# crontab -l
*/30 * * * * /usr/sbin/ntpdate  120.25.115.20 &> /dev/null
```

**3、所有节点添加主机名解析** 

```
[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.140.13	k8s-master01.linux.com k8s-master01
192.168.140.14	k8s-master02.linux.com k8s-master02
192.168.140.15	k8s-master03.linux.com k8s-master03
192.168.140.16	k8s-node01.linux.com k8s-node01
192.168.140.18	k8s-node02.linux.com k8s-node02
192.168.140.100	k8s-master-vip
```

 **4、所有节点添加docker软件仓库**

```
[root@k8s-master01 ~]# cat /etc/yum.repos.d/docker-ce.repo 
[docker-ce-stable]
name=Docker CE Stable - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/stable
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-stable-debuginfo]
name=Docker CE Stable - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/stable
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-stable-source]
name=Docker CE Stable - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/stable
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test]
name=Docker CE Test - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/test
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test-debuginfo]
name=Docker CE Test - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/test
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test-source]
name=Docker CE Test - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/test
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly]
name=Docker CE Nightly - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly-debuginfo]
name=Docker CE Nightly - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly-source]
name=Docker CE Nightly - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg
```

**5、所有节点添加kubernetes软件仓库**

```
[root@k8s-master01 ~]# cat /etc/yum.repos.d/kubernetes.repo 
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
```

**6、所有节点禁用swap**

```
# swapoff -a
# sysctl -w vm.swappiness=0
# sed  -i '/swap/s/^/#/' /etc/fstab
```

**7、所有节点调整系统资源限制**

```
[root@k8s-master01 ~]# ulimit -SHn 65535
```

```
[root@k8s-master01 ~]# vim /etc/security/limits.conf 
* soft nofile 655360
* hard nofile 131072
* soft nproc 655350
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
```

**8、所有节点升级系统，并重启**

```
# yum update 
# reboot
```

```
[root@k8s-master01 ~]# cat /etc/redhat-release 
CentOS Linux release 7.9.2009 (Core)

[root@k8s-master01 ~]# uname -r
3.10.0-1160.25.1.el7.x86_64

```

**三、内核参数调整**

**1、所有节点安装ipvs**

```
# yum install ipvsadm ipset sysstat conntrack libseccomp -y
```

**2、所有节点加载ipvs模块**

```
[root@k8s-master01 ~]# cat /etc/modules-load.d/ipvs.conf 
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack_ipv4
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
```

```
# systemctl enable --now systemd-modules-load.service
```

**3、调整内核参数**

```
[root@k8s-master01 ~]# cat /etc/sysctl.d/k8s.conf 
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
```

```
[root@k8s-master01 ~]# sysctl --system
```

**四、所有节点安装docker**

```
[root@k8s-master01 ~]# yum install -y docker-ce-19.03* 

[root@k8s-master01 ~]# systemctl start docker
[root@k8s-master01 ~]# systemctl enable docker
```

```
[root@k8s-master01 ~]# cat /etc/docker/daemon.json
{"registry-mirrors": ["http://f1361db2.m.daocloud.io"]}
 
[root@k8s-master01 ~]# systemctl restart docker
```

**五、所有节点安装kubeadm工具**

**1、安装kubeadm 1.20.7**

```
[root@k8s-master01 ~]# yum install -y kubeadm-1.20.7 kubelet-1.20.7 kubectl-1.20.7
```

**2、kubelet默认下载pause镜像地址为gcr.io，修改其为国内地址**    

```
[root@k8s-master01 ~]# cat /etc/sysconfig/kubelet          
KUBELET_EXTRA_ARGS="--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2"
```

**3、启动kubelet** 

```
[root@k8s-master01 ~]# systemctl enable kubelet
[root@k8s-master01 ~]# systemctl start kubelet
```

**六、在三个Master节点分别安装Haproxy和keepalived**

**1、安装相关软件**

```
# yum install -y haproxy keepalived
```

**2、所有Master节点配置haproxy提供负载均衡**

```
[root@k8s-master01 ~]# cat /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:16443
  bind 127.0.0.1:16443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.140.13:6443  check
  server k8s-master02	192.168.140.14:6443  check
  server k8s-master03	192.168.140.15:6443  check
```

**注意: 三个Master节点haproxy配置保持一致!!!**

**3、三个节点配置keepalived实现高可用**

**检查kube-apiserver状态脚本**

```
[root@k8s-master01 ~]# cat /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != "0" ]]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
```

**注意: 三个master节点keepalived配置优先级、状态区分**

**k8s-master01.linux.com**

```
[root@k8s-master01 ~]# cat /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass redhat
    }
    virtual_ipaddress {
        192.168.140.100
    }
    track_script {
       chk_apiserver
    }
}
```

**k8s-master02.linux.com**

```
[root@k8s-master02 ~]# cat /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass redhat
    }
    virtual_ipaddress {
        192.168.140.100
    }
    track_script {
       chk_apiserver
    }
}
```

**k8s-master03.linux.com**

```
[root@k8s-master03 ~]# cat /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass redhat
    }
    virtual_ipaddress {
        192.168.140.100
    }
    track_script {
       chk_apiserver
    }
}
```

**4、分别启动haproxy和keepalived**

```
[root@k8s-master01 ~]# systemctl enable haproxy keepalived
[root@k8s-master01 ~]# systemctl start haproxy keepalived
```

**5、测试VIP工作正常，可正常通信**

```
[root@k8s-master01 ~]# ping 192.168.140.100
PING 192.168.140.100 (192.168.140.100) 56(84) bytes of data.
64 bytes from 192.168.140.100: icmp_seq=1 ttl=64 time=0.075 ms
64 bytes from 192.168.140.100: icmp_seq=2 ttl=64 time=0.040 ms
64 bytes from 192.168.140.100: icmp_seq=3 ttl=64 time=0.048 ms
```

**七、kubernetes集群初始化**

**1、在k8s-master01节点准备初始化集群文件**

```
[root@k8s-master01 ~]# cat new.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 7t2weq.bjbawausm0jaxury
  ttl: 96h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.140.13
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master01
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  certSANs:
  - 192.168.140.100
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.140.100:16443
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.20.7
networking:
  dnsDomain: cluster.local
  podSubnet: 172.168.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

**2、事先在所有Master节点下载需要的镜像**

```
[root@k8s-master01 ~]# kubeadm config images pull --config /root/new.yaml
```

**3、在k8s-master01节点初始化集群**

```
[root@k8s-master01 ~]# kubeadm init --config /root/new.yaml  --upload-certs



[init] Using Kubernetes version: v1.20.7
[preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
	[WARNING Hostname]: hostname "k8s-master01" could not be reached
	[WARNING Hostname]: hostname "k8s-master01": lookup k8s-master01 on 114.114.114.114:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.183.10 192.168.183.100]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master01 localhost] and IPs [192.168.183.10 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master01 localhost] and IPs [192.168.183.10 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "admin.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 23.034911 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.20" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
8f86add4cce0bb5ba1ad55ce5bd2da55fcbebe6d720570105c0fbbcd507091e3
[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the labels "node-role.kubernetes.io/master=''" and "node-role.kubernetes.io/control-plane='' (deprecated)"
[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 7t2weq.bjbawausm0jaxury
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.183.100:16443 --token 7t2weq.bjbawausm0jaxury \
    --discovery-token-ca-cert-hash sha256:840ee303f9bfa42f5e2d9f20fb3c2c9ab35b62b01201c3e778153f1de1e0e9cd \
    --control-plane --certificate-key 8f86add4cce0bb5ba1ad55ce5bd2da55fcbebe6d720570105c0fbbcd507091e3

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.183.100:16443 --token 7t2weq.bjbawausm0jaxury \
    --discovery-token-ca-cert-hash sha256:840ee303f9bfa42f5e2d9f20fb3c2c9ab35b62b01201c3e778153f1de1e0e9cd 
```

**4、在k8s-master01节点设置环境变量，便于访问操作集群** 

```
[root@k8s-master01 ~]# vim /root/.bashrc 
export KUBECONFIG=/etc/kubernetes/admin.conf

[root@k8s-master01 ~]# source /root/.bashrc
```

```
[root@k8s-master01 ~]# kubectl get nodes
NAME           STATUS     ROLES                  AGE    VERSION
k8s-master01   NotReady   control-plane,master   102s   v1.20.7
```

**八、将其他master节点加入集群**

```
kubeadm join 192.168.140.100:16443 --token 7t2weq.bjbawausm0jaxury     --discovery-token-ca-cert-hash sha256:978fa35c37b8813d4d47a27747a6d88b77372d9eedfe5667da13c436cfa2a127     --control-plane --certificate-key 16a7bedd682685bf70a49d59ce1f10627c9219d5387f6476c26e481e400422f0
```

**九、将其他node节点加入集群** 

```
# kubeadm join 192.168.140.100:16443 --token 7t2weq.bjbawausm0jaxury     --discovery-token-ca-cert-hash sha256:978fa35c37b8813d4d47a27747a6d88b77372d9eedfe5667da13c436cfa2a127
```

```
[root@k8s-master01 ~]# kubectl get nodes
NAME                     STATUS     ROLES                  AGE     VERSION
k8s-master01             NotReady   control-plane,master   3m43s   v1.20.7
k8s-master02.linux.com   NotReady   control-plane,master   93s     v1.20.7
k8s-master03.linux.com   NotReady   control-plane,master   44s     v1.20.7
k8s-node01.linux.com     NotReady   <none>                 33s     v1.20.7
k8s-node02.linux.com     NotReady   <none>                 9s      v1.20.7
```

**十、部署calico网络**

该操作只需要在k8s-master01节点执行

**1、将k8s-ha-install安装源文件上传到服务器**

```
[root@k8s-master01 calico]# pwd
/root/k8s-ha-install/calico
```

**2、修改calico-etcd.yaml配置，添加etcd数据库连接地址**

```
[root@k8s-master01 calico]# sed -i 's#etcd_endpoints: "http://<ETCD_IP>:<ETCD_PORT>"#etcd_endpoints: "https://192.168.140.13:2379,https://192.168.140.14:2379,https://192.168.140.15:2379"#g' calico-etcd.yaml
```

**3、在文件中添加证书相关信息**

```
# ETCD_CA=`cat /etc/kubernetes/pki/etcd/ca.crt | base64 | tr -d '\n'`
# ETCD_CERT=`cat /etc/kubernetes/pki/etcd/server.crt | base64 | tr -d '\n'`
# ETCD_KEY=`cat /etc/kubernetes/pki/etcd/server.key | base64 | tr -d '\n'`
# sed -i "s@# etcd-key: null@etcd-key: ${ETCD_KEY}@g; s@# etcd-cert: null@etcd-cert: ${ETCD_CERT}@g; s@# etcd-ca: null@etcd-ca: ${ETCD_CA}@g" calico-etcd.yaml
```

```
# sed -i 's#etcd_ca: ""#etcd_ca: "/calico-secrets/etcd-ca"#g; s#etcd_cert: ""#etcd_cert: "/calico-secrets/etcd-cert"#g; s#etcd_key: "" #etcd_key: "/calico-secrets/etcd-key" #g' calico-etcd.yaml
```

**4、在文件中修改pod网段信息**

```
# sed -i 's@# - name: CALICO_IPV4POOL_CIDR@- name: CALICO_IPV4POOL_CIDR@g; s@#   value: "172.168.0.0/16"@  value: '"${POD_SUBNET}"'@g' calico-etcd.yaml
```

**5、部署caclico网络** 

```
[root@k8s-master01 calico]# kubectl apply -f calico-etcd.yaml
```

**6、等待kube-system命名空间中的所有pod均为running状态，再次查看集群节点**

```
[root@k8s-master01 calico]# kubectl get nodes
NAME                     STATUS   ROLES                  AGE     VERSION
k8s-master01             Ready    control-plane,master   11m     v1.20.7
k8s-master02.linux.com   Ready    control-plane,master   9m44s   v1.20.7
k8s-master03.linux.com   Ready    control-plane,master   8m55s   v1.20.7
k8s-node01.linux.com     Ready    <none>                 8m44s   v1.20.7
k8s-node02.linux.com     Ready    <none>                 8m20s   v1.20.7
```

**十一、部署Metric，用于对node节点进行监控、CPU内存资源采集**

**1、将frontend证书拷贝到所有node节点**

```
[root@k8s-master01 ~]#  scp /etc/kubernetes/pki/front-proxy-ca.crt 192.168.140.16:/etc/kubernetes/pki/front-proxy-ca.crt
[root@k8s-master01 ~]#  scp /etc/kubernetes/pki/front-proxy-ca.crt 192.168.140.18:/etc/kubernetes/pki/front-proxy-ca.crt
```

**2、部署metric**

```
[root@k8s-master01 metrics-server-0.4.x-kubeadm]# pwd
/root/k8s-ha-install/metrics-server-0.4.x-kubeadm

[root@k8s-master01 metrics-server-0.4.x-kubeadm]#  kubectl  create -f comp.yaml 
```

**3、测试metric工作正常**

```
[root@k8s-master01 ~]# kubectl top node
NAME                     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-master01             166m         4%     1054Mi          27%       
k8s-master02.linux.com   173m         4%     1126Mi          29%       
k8s-master03.linux.com   140m         3%     924Mi           24%       
k8s-node01.linux.com     102m         2%     647Mi           16%       
k8s-node02.linux.com     84m          2%     679Mi           18%   
```

**十二、部署dashboard**

**1、部署dashboard**

```
[root@k8s-master01 dashboard]# cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
```

**2、修改dashboard服务类型为nodePort**

```
[root@k8s-master01 dashboard]#  kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard

     25     port: 443
     26     protocol: TCP
     27     targetPort: 8443
     28   selector:
     29     k8s-app: kubernetes-dashboard
     30   sessionAffinity: None
     31   type: NodePort

```

**3、查看dashboard服务运行**

```
[root@k8s-master01 ~]# kubectl get svc -n kubernetes-dashboard
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   10.107.41.129   <none>        8000/TCP        8d
kubernetes-dashboard        NodePort    10.108.189.40   <none>        443:32372/TCP   8d
```

**4、访问dashboard**

**![clipboard](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\\clipboard.png)**

**5、获取dashboard相关用户登录token**

```
[root@k8s-master01 ~]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-rmdqw
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: bcefc3d1-f91b-4b63-8287-043b8ea70a07

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1066 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjJqbGViVFVaR3RRcThHQzhzWFFDdjBnY3RHcUdGSGdia0Q1ZldjbEZ1aDQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXJtZHF3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiY2VmYzNkMS1mOTFiLTRiNjMtODI4Ny0wNDNiOGVhNzBhMDciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.NWuzXvlgVhyIGbQWhnbu16rRRE9Y-VgngP2kO9YN8_6pfGx4BiFWRTQx_BpTby532mCpiHyJPTknScjCzgMwHHiNyNwWy474-MEb8LJ1NGf3tvDh4M_GkdAqxfN2qRMhRhbJ0xH98T4kaJfWcmfd-HJGnkNVAfyFOeOJgAYinQMuavrotRLY0iDeTmGV2Dzb0vFMK3RIBmCwsz82RJvsA1j9Mxcp3GUaDrC5nkPMWsUPY7r9fnf9jX9zVpxHPcCtonxMwZKubBy6gxKK9aBka7iTrthH1j8djAsFLRAIdgoJE_nydzSJEaMrLn69vy10pQawq3K1ABMYS4EyF0U1aw
```

**6、登录成功后，dashboard显示界面如下**

![clipboard (1)](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\clipboard (1).png)

**十三、查看集群状态**

**1、查看所有节点**

```
[root@k8s-master01 ~]# kubectl get nodes
NAME                     STATUS   ROLES                  AGE   VERSION
k8s-master01             Ready    control-plane,master   8d    v1.20.7
k8s-master02.linux.com   Ready    control-plane,master   8d    v1.20.7
k8s-master03.linux.com   Ready    control-plane,master   8d    v1.20.7
k8s-node01.linux.com     Ready    <none>                 8d    v1.20.7
k8s-node02.linux.com     Ready    <none>                 8d    v1.20.7
```

**2、查看集群所有组件**

```
[root@k8s-master01 ~]# kubectl get pods -n kube-system
NAME                                             READY   STATUS    RESTARTS   AGE
calico-kube-controllers-5f6d4b864b-s4bsq         1/1     Running   5          8d
calico-node-6j9cr                                1/1     Running   5          8d
calico-node-bh2c6                                1/1     Running   5          8d
calico-node-fzd4t                                1/1     Running   7          8d
calico-node-qds9m                                1/1     Running   8          8d
calico-node-x8s4c                                1/1     Running   5          8d
coredns-54d67798b7-5pv7g                         1/1     Running   5          8d
coredns-54d67798b7-pfvlr                         1/1     Running   5          8d
etcd-k8s-master01                                1/1     Running   6          8d
etcd-k8s-master02.linux.com                      1/1     Running   5          8d
etcd-k8s-master03.linux.com                      1/1     Running   5          8d
kube-apiserver-k8s-master01                      1/1     Running   17         8d
kube-apiserver-k8s-master02.linux.com            1/1     Running   5          8d
kube-apiserver-k8s-master03.linux.com            1/1     Running   8          8d
kube-controller-manager-k8s-master01             1/1     Running   8          8d
kube-controller-manager-k8s-master02.linux.com   1/1     Running   5          8d
kube-controller-manager-k8s-master03.linux.com   1/1     Running   5          8d
kube-proxy-4skph                                 1/1     Running   5          8d
kube-proxy-g8kvq                                 1/1     Running   5          8d
kube-proxy-j7n87                                 1/1     Running   5          8d
kube-proxy-ldw4k                                 1/1     Running   5          8d
kube-proxy-lt876                                 1/1     Running   5          8d
kube-scheduler-k8s-master01                      1/1     Running   8          8d
kube-scheduler-k8s-master02.linux.com            1/1     Running   5          8d
kube-scheduler-k8s-master03.linux.com            1/1     Running   5          8d
metrics-server-545b8b99c6-jcj2n                  1/1     Running   6          8d
```

